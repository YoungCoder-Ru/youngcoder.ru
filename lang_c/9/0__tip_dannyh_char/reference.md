## Дополнительные материалы

**1\.** Очень интересная [заметка про красоту и изящество таблицы ASCII](https://habr.com/ru/companies/ruvds/articles/831138/)

**2\.** Обратите внимание, что какой бы из =символьных типов= мы не использовали (`char`, `unsigned char` или `signed char`), значения от `0` до `127`, соответствующие кодам символов из основной таблицы ASCII, всегда полностью умещаются в этом типе и не вызывают переполнений или каких-то проблем. Т.е. если мы хотим работать с базовым набором ASCII символов, то нам подходит любой из этих трёх типов. 

**3\.** Кодировка ASCII -- это, скажем так, "золотой стандарт" в мире компьютерной техники! Текст, записанный в этой кодировке можно будет прочитать буквально на любом современном компьютере. Кроме того, все другие современные кодировки устроены таким образом, что их первые 128 символов такие же, как в кодировке ASCII. 

**4\.** Обратите внимание, что фактически ASCII -- это семибитная кодировка, т.к. нужно закодировать всего лишь `128` символов (`0-127`). Зачастую используют расширенную восьмибитную кодировку ASCII, где коды со `128` по `255` используются для национальных символов. Например, кодировки CP-1251, KOI8-R для кириллицы. 

Т.к. не было никаких договорённостей о том, какие символы включать в "региональную часть" кириллических расширений ASCII, то, например, буквы А-Я в CP-1251 расположены в алфавитном порядке со `192` по `223` позиции (буква Ё на `168`). Эти же буквы в KOI8-R расположены не в алфавитном порядке (а скорее в фонетическом порядке, похожем на латинский алфавит) на позициях с `224` по `255` (буква Ё на 161). Поэтому использование неправильной кодировки может превращать текст в непонятные "кракозябры".


**5\.** Вы скорее всего знаете, что `1` байт равен `8` битам. Это действительно так для подавляющего большинства современных компьютерных систем. Однако исторически это не всегда было так. 

На заре развития компьютерной техники даже самого понятия байт не существовало. Потом его ввёл в оборот В. Бухгольц, но не было никакой договорённости о том, что байт -- это всегда `8` бит.

Сейчас это уже можно считать стандартом де-факто. Но любопытно, что в стандарте языка Си определяется специальная =именованная константа= `CHAR_BIT` (количество бит в байте), значение которой должно быть не меньше `8`, но может быть и больше. 

На практике в современных системах `CHAR_BIT` всегда равен `8`. Можете убедиться в этом самостоятельно, посмотрев файл `limits.h`.

Заметим, что `8` является степенью двойки. Поэтому очень удобно описывать `1` байт в виде двух шестнадцатеричных символов, где каждый символ отвечает за `4` бита.

Например, символ `'C'` имеет код `67`, что в двоичной системе `0100 0011` (для наглядности добавил пробел между квартетами) или `5 2` в шестнадцатеричной системе счисления.

Для символа `'+'` получим: 
- `43`- десятичная
- `00101011` - двоичная 
- `2B` - шестнадцатеричная.

Представление байта в виде двух шестнадцатеричных цифр используется очень-очень часто.

**6\.** Любопытный факт. Символьный литерал, например, `'C'` имеет тип `int`, а не `char`, как можно было бы подумать. В этом легко убедиться, если запустить следующую программу:

```c
#include <stdio.h>

int main(void)
{
        printf("%d\n", sizeof('C'));

        return(0);
}
```

Если бы `'C'` имел тип `char` мы бы увидели в консоли значение `1`, т.к. размер типа `char` один байт. А мы видим `4`, что говорит нам о том, что это `int`. Получается, что каждый раз, когда мы пишем что-то вроде `char ch = 'D';` происходит преобразование типа. =)